{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 2, Sprint 1, Module 3*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "- Do one-hot encoding of categorical features\n",
    "- Do univariate feature selection\n",
    "- Express and explain the intuition and interpretation of Ridge Regression\n",
    "- Use sklearn.linear_model.Ridge to fit and interpret Ridge Regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "You can work locally (follow the [local setup instructions](https://lambdaschool.github.io/ds/unit2/local/)) or on Colab (run the code cell below).\n",
    "\n",
    "Libraries:\n",
    "- category_encoders\n",
    "- matplotlib\n",
    "- numpy\n",
    "- pandas\n",
    "- scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "# If you're on Colab:\n",
    "if 'google.colab' in sys.modules:\n",
    "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Applied-Modeling/master/data/'\n",
    "    !pip install category_encoders==2.*\n",
    "\n",
    "# If you're working locally:\n",
    "else:\n",
    "    DATA_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do one-hot encoding of categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the NYC apartment rental listing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read New York City apartment rental listing data\n",
    "df = pd.read_csv(DATA_PATH+'apartments/renthop-nyc.csv')\n",
    "assert df.shape == (49352, 34)\n",
    "\n",
    "# Remove the most extreme 1% prices,\n",
    "# the most extreme .1% latitudes, &\n",
    "# the most extreme .1% longitudes\n",
    "df = df[(df['price'] >= np.percentile(df['price'], 0.5)) & \n",
    "        (df['price'] <= np.percentile(df['price'], 99.5)) & \n",
    "        (df['latitude'] >= np.percentile(df['latitude'], 0.05)) & \n",
    "        (df['latitude'] < np.percentile(df['latitude'], 99.95)) &\n",
    "        (df['longitude'] >= np.percentile(df['longitude'], 0.05)) & \n",
    "        (df['longitude'] <= np.percentile(df['longitude'], 99.95))]\n",
    "\n",
    "# Do train/test split\n",
    "# Use data from April & May 2016 to train\n",
    "# Use data from June 2016 to test\n",
    "df['created'] = pd.to_datetime(df['created'], infer_datetime_format=True)\n",
    "cutoff = pd.to_datetime('2016-06-01')\n",
    "train = df[df.created < cutoff]\n",
    "test  = df[df.created >= cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some columns are numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some columns are _not_ numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the relationship between `interest_level` and `price`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interest Level seems like a useful, predictive feature. But it's a string — and our scikit-learn models expect all inputs to be numbers. \n",
    "\n",
    "So, we can \"one-hot encode\" the feature.\n",
    "\n",
    "To go from this:\n",
    "\n",
    "| interest_level |\n",
    "|----------------|\n",
    "| high           |\n",
    "| medium         |\n",
    "| low            |\n",
    "\n",
    "\n",
    "To this:\n",
    "\n",
    "| interest_level_high | interest_level_medium | interest_level_low |\n",
    "|---------------------|-----------------------|--------------------|\n",
    "| 1                   | 0                     | 0                  |\n",
    "| 0                   | 1                     | 0                  |\n",
    "| 0                   | 0                     | 1                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"One-hot encoding\" adds a dimension for each unique value of each categorical feature. So, it may not be a good choice for \"high cardinality\" categoricals that have dozens, hundreds, or thousands of unique values. \n",
    "\n",
    "[Cardinality](https://simple.wikipedia.org/wiki/Cardinality) means the number of unique values that a feature has:\n",
    "> In mathematics, the cardinality of a set means the number of its elements. For example, the set A = {2, 4, 6} contains 3 elements, and therefore A has a cardinality of 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other non-numeric columns have high cardinality. So let's exclude them from our features for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what `X_train` looks like **before** encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [OneHotEncoder](https://contrib.scikit-learn.org/categorical-encoding/onehot.html) from the [category_encoders](https://github.com/scikit-learn-contrib/categorical-encoding) library to encode any non-numeric features. (In this case, it's just `interest_level`.)\n",
    "\n",
    "- Use the **`fit_transform`** method on the **train** set\n",
    "- Use the **`transform`** method on **validation / test** sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what it looks like **after:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your assignment, you will do one-hot encoding of categorical features with feasible cardinality, using the category_encoders library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do univariate feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous assignment quoted Wikipedia on [Feature Engineering](https://en.wikipedia.org/wiki/Feature_engineering):\n",
    "\n",
    "> \"Some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.\" — Pedro Domingos, [\"A Few Useful Things to Know about Machine Learning\"](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "\n",
    "> \"Coming up with features is difficult, time-consuming, requires expert knowledge. 'Applied machine learning' is basically feature engineering.\" — Andrew Ng, [Machine Learning and AI via Brain simulations](https://forum.stanford.edu/events/2011/2011slides/plenary/2011plenaryNg.pdf) \n",
    "\n",
    "> Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. \n",
    "\n",
    "Pedro Domingos says, \"the most important factor is the **features used**.\"\n",
    "\n",
    "This includes not just **Feature Engineering** (making new features, representing features in new ways) but also **Feature Selection** (choosing which features to include and which to exclude).\n",
    "\n",
    "There are _many_ specific tools and techniques for feature selection.\n",
    "\n",
    "- Today we'll try [scikit-learn's `SelectKBest` transformer](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection), for \"univariate, forward selection.\"\n",
    "- Later we'll try another technique, [\"permutation importance\"](https://www.kaggle.com/dansbecker/permutation-importance)\n",
    "- If you want to explore even more options, here are some good resources!\n",
    "  - [scikit-learn's User Guide for Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "  - [mlxtend](http://rasbt.github.io/mlxtend/) library\n",
    "  - scikit-learn-contrib libraries: [boruta_py](https://github.com/scikit-learn-contrib/boruta_py) & [stability-selection](https://github.com/scikit-learn-contrib/stability-selection)\n",
    "  - [_Feature Engineering and Selection_](http://www.feat.engineering/) by Kuhn & Johnson.\n",
    "\n",
    "\n",
    "My general recommendation is:\n",
    "\n",
    "> Predictive accuracy on test sets is the criterion for how good the model is. — Leo Breiman, [\"Statistical Modeling: The Two Cultures\"](https://projecteuclid.org/download/pdf_1/euclid.ss/1009213726)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's engineer a few more features to select from. This is a partial example solution from the previous assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(X):\n",
    "    \n",
    "    # Avoid SettingWithCopyWarning\n",
    "    X = X.copy()\n",
    "    \n",
    "    # How many total perks does each apartment have?\n",
    "    perk_cols = ['elevator', 'cats_allowed', 'hardwood_floors', 'dogs_allowed',\n",
    "                 'doorman', 'dishwasher', 'no_fee', 'laundry_in_building',\n",
    "                 'fitness_center', 'pre-war', 'laundry_in_unit', 'roof_deck',\n",
    "                 'outdoor_space', 'dining_room', 'high_speed_internet', 'balcony',\n",
    "                 'swimming_pool', 'new_construction', 'exclusive', 'terrace', \n",
    "                 'loft', 'garden_patio', 'common_outdoor_space', \n",
    "                 'wheelchair_access']\n",
    "    X['perk_count'] = X[perk_cols].sum(axis=1)\n",
    "\n",
    "    # Are cats or dogs allowed?\n",
    "    X['cats_or_dogs'] = (X['cats_allowed']==1) | (X['dogs_allowed']==1)\n",
    "\n",
    "    # Are cats and dogs allowed?\n",
    "    X['cats_and_dogs'] = (X['cats_allowed']==1) & (X['dogs_allowed']==1)\n",
    "\n",
    "    # Total number of rooms (beds + baths)\n",
    "    X['rooms'] = X['bedrooms'] + X['bathrooms']\n",
    "\n",
    "    return X\n",
    "\n",
    "X_train = engineer_features(X_train)\n",
    "X_test = engineer_features(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Could we try every possible feature combination?\n",
    "\n",
    "The number of [combinations](https://en.wikipedia.org/wiki/Combination) is shocking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many features do we have currently?\n",
    "features = X_train.columns\n",
    "n = len(features)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many ways to choose 1 to n features?\n",
    "from math import factorial\n",
    "\n",
    "def n_choose_k(n, k):\n",
    "    return factorial(n)/(factorial(k)*factorial(n-k))\n",
    "\n",
    "combinations = sum(n_choose_k(n,k) for k in range(1,n+1))\n",
    "print(f'{combinations:,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't try every possible combination, but we can try some. For example, we can use univariate statistical tests to measure the correlation between each feature and the target, and select the k best features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the [Scikit-Learn User Guide on Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "In your assignment, you will do feature selection with SelectKBest.\n",
    "\n",
    "You'll go back to our other New York City real estate dataset. Instead of predicting apartment rents, you'll predict property sales prices. But not just for condos in Tribeca. Instead, you'll predict property sales prices for One Family Dwellings, where the sale price was more than 100 thousand and less than 2 million.\n",
    "\n",
    "If you run the above code cell with your assignment dataset, you will probably get some shocking results like these:\n",
    "\n",
    "```\n",
    "1 features\n",
    "Test MAE: $183,641 \n",
    "\n",
    "2 features\n",
    "Test MAE: $182,569 \n",
    "\n",
    "3 features\n",
    "Test MAE: $182,569 \n",
    "\n",
    "4 features\n",
    "Test MAE: $183,441 \n",
    "\n",
    "5 features\n",
    "Test MAE: $186,532 \n",
    "\n",
    "6 features\n",
    "Test MAE: $176,121 \n",
    "\n",
    "7 features\n",
    "Test MAE: $177,001 \n",
    "\n",
    "8 features\n",
    "Test MAE: $176,707 \n",
    "\n",
    "9 features\n",
    "Test MAE: $170,969 \n",
    "\n",
    "10 features\n",
    "Test MAE: $170,977 \n",
    "\n",
    "11 features\n",
    "Test MAE: $170,507 \n",
    "\n",
    "12 features\n",
    "Test MAE: $162,301 \n",
    "\n",
    "13 features\n",
    "Test MAE: $163,559 \n",
    "\n",
    "14 features\n",
    "Test MAE: $162,562 \n",
    "\n",
    "15 features\n",
    "Test MAE: $162,550 \n",
    "\n",
    "16 features\n",
    "Test MAE: $162,678 \n",
    "\n",
    "17 features\n",
    "Test MAE: $162,419 \n",
    "\n",
    "18 features\n",
    "Test MAE: $162,177 \n",
    "\n",
    "19 features\n",
    "Test MAE: $162,177 \n",
    "\n",
    "20 features\n",
    "Test MAE: $157,893 \n",
    "\n",
    "21 features\n",
    "Test MAE: $157,966 \n",
    "\n",
    "22 features\n",
    "Test MAE: $157,966 \n",
    "\n",
    "23 features\n",
    "Test MAE: $157,966 \n",
    "\n",
    "24 features\n",
    "Test MAE: $157,630 \n",
    "\n",
    "25 features\n",
    "Test MAE: $157,580 \n",
    "\n",
    "26 features\n",
    "Test MAE: $25,968,990,575,776,280 \n",
    "\n",
    "27 features\n",
    "Test MAE: $157,550 \n",
    "\n",
    "28 features\n",
    "Test MAE: $87,300,193,986,380,608 \n",
    "\n",
    "29 features\n",
    "Test MAE: $158,491 \n",
    "\n",
    "30 features\n",
    "Test MAE: $17,529,140,644,990,770 \n",
    "\n",
    "31 features\n",
    "Test MAE: $24,191,458,933,688,856 \n",
    "\n",
    "32 features\n",
    "Test MAE: $15,214,122,471,992,104 \n",
    "\n",
    "33 features\n",
    "Test MAE: $15,539,731,847,001,626 \n",
    "\n",
    "34 features\n",
    "Test MAE: $26,823,915,969,200,480 \n",
    "\n",
    "35 features\n",
    "Test MAE: $3,813,290,272,870,121 \n",
    "\n",
    "36 features\n",
    "Test MAE: $157,900 \n",
    "\n",
    "37 features\n",
    "Test MAE: $157,900 \n",
    "\n",
    "38 features\n",
    "Test MAE: $158,911 \n",
    "\n",
    "39 features\n",
    "Test MAE: $9,101,846,282,581,472 \n",
    "\n",
    "40 features\n",
    "Test MAE: $1,424,168,120,777,820 \n",
    "\n",
    "41 features\n",
    "Test MAE: $158,261 \n",
    "\n",
    "42 features\n",
    "Test MAE: $158,261 \n",
    "\n",
    "43 features\n",
    "Test MAE: $4,784,333,158,313,152 \n",
    "\n",
    "44 features\n",
    "Test MAE: $1,329,759,264,341,476 \n",
    "\n",
    "45 features\n",
    "Test MAE: $158,451 \n",
    "\n",
    "46 features\n",
    "Test MAE: $158,450 \n",
    "\n",
    "47 features\n",
    "Test MAE: $158,450 \n",
    "\n",
    "48 features\n",
    "Test MAE: $1,331,383,815,682,658 \n",
    "\n",
    "49 features\n",
    "Test MAE: $1,504,319,420,789,134 \n",
    "\n",
    "50 features\n",
    "Test MAE: $2,285,927,437,866,492\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did the error blow up with 26 features? We can look at the coefficients of the selected features:\n",
    "\n",
    "```\n",
    "BLOCK                                                  -97,035\n",
    "ZIP_CODE                                              -152,985\n",
    "COMMERCIAL_UNITS                    -4,115,324,664,197,034,496\n",
    "TOTAL_UNITS                         -2,872,516,607,892,124,160\n",
    "GROSS_SQUARE_FEET                                      123,739\n",
    "BOROUGH_3                                              146,876\n",
    "BOROUGH_4                                              197,262\n",
    "BOROUGH_2                                              -55,917\n",
    "BOROUGH_5                                              -84,107\n",
    "NEIGHBORHOOD_OTHER                                      56,036\n",
    "NEIGHBORHOOD_FLUSHING-NORTH                             63,394\n",
    "NEIGHBORHOOD_FOREST HILLS                               46,411\n",
    "NEIGHBORHOOD_BOROUGH PARK                               29,915\n",
    "TAX_CLASS_AT_PRESENT_1D               -545,831,543,721,722,112\n",
    "BUILDING_CLASS_AT_PRESENT_A5                            -1,673\n",
    "BUILDING_CLASS_AT_PRESENT_A3             5,516,361,218,128,626\n",
    "BUILDING_CLASS_AT_PRESENT_S1         1,735,885,668,110,473,216\n",
    "BUILDING_CLASS_AT_PRESENT_A6           -25,974,113,243,185,788\n",
    "BUILDING_CLASS_AT_PRESENT_A8          -760,608,646,332,801,664\n",
    "BUILDING_CLASS_AT_PRESENT_S0           941,854,072,479,442,176\n",
    "BUILDING_CLASS_AT_TIME_OF_SALE_A5                      -24,599\n",
    "BUILDING_CLASS_AT_TIME_OF_SALE_A3       -5,516,361,218,111,506\n",
    "BUILDING_CLASS_AT_TIME_OF_SALE_S1    4,253,055,231,847,939,072\n",
    "BUILDING_CLASS_AT_TIME_OF_SALE_A6       25,974,113,243,176,404\n",
    "BUILDING_CLASS_AT_TIME_OF_SALE_A8     -541,733,439,448,260,800\n",
    "BUILDING_CLASS_AT_TIME_OF_SALE_S0      990,851,394,820,416,512\n",
    "```\n",
    "\n",
    "These were the coefficients that minimized the sum of squared errors in the training set. But this model has become too complex, with extreme coefficients that can lead to extreme predictions on new unseen data. \n",
    "\n",
    "This linear model needs _regularization._ Ridge Regression to the rescue!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Express and explain the intuition and interpretation of Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our other New York City dataset, to demonstrate regularization with Ridge Regresson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import display, HTML\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Try a range of alpha parameters for Ridge Regression.\n",
    "\n",
    "# The scikit-learn docs explain, \n",
    "# alpha : Regularization strength; must be a positive float. Regularization \n",
    "# improves the conditioning of the problem and reduces the variance of the \n",
    "# estimates. Larger values specify stronger regularization.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "for alpha in [10**1, 10**2, 10**3, 10**4, 10**5, 10**6]:\n",
    "    \n",
    "    # Scale data before doing Ridge Regression\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Fit Ridge Regression model\n",
    "    display(HTML(f'Ridge Regression, with alpha={alpha}'))\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Get Test MAE\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    display(HTML(f'Test Mean Absolute Error: ${mae:,.0f}'))\n",
    "    \n",
    "    # Plot coefficients\n",
    "    coefficients = pd.Series(model.coef_, X_train.columns)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    coefficients.sort_values().plot.barh(color='grey')\n",
    "    plt.xlim(-400,700)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's back up — _way_ up!\n",
    "\n",
    "Data science depends on math, and math is generally focused on situations where:\n",
    "\n",
    "1. a solution exists,\n",
    "2. the solution is unique,\n",
    "3. the solution's behavior changes continuously with the initial conditions.\n",
    "\n",
    "These are known as [well-posed problems](https://en.wikipedia.org/wiki/Well-posed_problem), and are the sorts of assumptions so core in traditional techniques that it is easy to forget about them. But they do matter, as there can be exceptions:\n",
    "\n",
    "1. no solution - e.g. no $x$ such that $Ax = b$\n",
    "2. multiple solutions - e.g. several $x_1, x_2, ...$ such that $Ax = b$\n",
    "3. \"chaotic\" systems - situations where small changes in initial conditions interact and reverberate in essentially unpredictable ways - for instance, the difficulty in longterm predictions of weather (N.B. not the same thing as longterm predictions of *climate*) - you can think of this as models that fail to generalize well, because they overfit on the training data (the initial conditions)\n",
    "\n",
    "Problems suffering from the above are called ill-posed problems. Relating to linear algebra and systems of equations, the only truly well-posed problems are those with a single unique solution.\n",
    "\n",
    "![Intersecting lines](https://upload.wikimedia.org/wikipedia/commons/c/c0/Intersecting_Lines.svg)\n",
    "\n",
    "Think for a moment - what would the above plot look like if there was no solution? If there were multiple solutions? And how would that generalize to higher dimensions?\n",
    "\n",
    "A lot of what you covered with linear regression was about getting matrices into the right shape for them to be solvable in this sense. But some matrices just won't submit to this, and other problems may technically \"fit\" linear regression but still be violating the above assumptions in subtle ways.\n",
    "\n",
    "[Overfitting](https://en.wikipedia.org/wiki/Overfitting) is in some ways a special case of this - an overfit model uses more features/parameters than is \"justified\" by the data (essentially by the *dimensionality* of the data, as measured by $n$ the number of observations). As the number of features approaches the number of observations, linear regression still \"works\", but it starts giving fairly perverse results. In particular, it results in a model that fails to *generalize* - and so the core goal of prediction and explanatory power is undermined.\n",
    "\n",
    "How is this related to well and ill-posed problems? It's not clearly a no solution or multiple solution case, but it does fall in the third category - overfitting results in fitting to the \"noise\" in the data, which means the particulars of one random sample or another (different initial conditions )will result in dramatically different models.\n",
    "\n",
    "### Stop and think - what are ways to address these issues?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along\n",
    "\n",
    "Let's examine in the context of another famous housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "boston = load_boston()\n",
    "boston.data = scale(boston.data)  # Very helpful for regularization!\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['Price'] = boston.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Let's try good old least squares!\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = df.drop('Price', axis='columns')\n",
    "y = df.Price\n",
    "\n",
    "lin_reg = LinearRegression().fit(X, y)\n",
    "mean_squared_error(y, lin_reg.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems like a pretty good score, but...\n",
    "\n",
    "![Kitchen Sink](https://i.imgur.com/ZZxqhT1.jpg)\n",
    "\n",
    "Chances are this doesn't generalize very well. You can verify this by splitting the data to properly test model validity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=43)\n",
    "lin_reg_split = LinearRegression().fit(X_train, y_train)\n",
    "print(mean_squared_error(y, lin_reg_split.predict(X)))\n",
    "print(mean_squared_error(y_test, lin_reg_split.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! 💥\n",
    "\n",
    "### What can we do?\n",
    "\n",
    "- Use fewer features - sure, but it can be a lot of work to figure out *which* features, and (in cases like this) there may not be any good reason to really favor some features over another.\n",
    "- Get more data! This is actually a pretty good approach in tech, since apps generate lots of data all the time (and we made this situation by artificially constraining our data). But for case studies, existing data, etc. it won't work.\n",
    "- **Regularize!**\n",
    "\n",
    "### Regularization just means \"add bias\"\n",
    "\n",
    "OK, there's a bit more to it than that. But that's the core intuition - the problem is the model working \"too well\", so fix it by making it harder for the model!\n",
    "\n",
    "It may sound strange - a technique that is purposefully \"worse\" - but in certain situations, it can really get results.\n",
    "\n",
    "What's bias? In the context of statistics and machine learning, bias is when a predictive model fails to identify relationships between features and the output. In a word, bias is *underfitting*.\n",
    "\n",
    "We want to add bias to the model because of the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) - variance is the sensitivity of a model to the random noise in its training data (i.e. *overfitting*), and bias and variance are naturally (inversely) related. Increasing one will always decrease the other, with regards to the overall generalization error (predictive accuracy on unseen data).\n",
    "\n",
    "Visually, the result looks like this:\n",
    "\n",
    "![Regularization example plot](https://upload.wikimedia.org/wikipedia/commons/0/02/Regularization.svg)\n",
    "\n",
    "The blue line is overfit, using more dimensions than are needed to explain the data and so much of the movement is based on noise and won't generalize well. The green line still fits the data, but is less susceptible to the noise - depending on how exactly we parameterize \"noise\" we may throw out actual correlation, but if we balance it right we keep that signal and greatly improve generalizability.\n",
    "\n",
    "### Look carefully at the above plot and think of ways you can quantify the difference between the blue and green lines..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use sklearn.linear_model.Ridge to fit and interpret Ridge Regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We'll use scikit-learn's implementation of Ridge Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with regularization via ridge regression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_reg = Ridge().fit(X, y)\n",
    "mean_squared_error(y, ridge_reg.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The score is a bit worse than OLS - but that's expected (we're adding bias)\n",
    "# Let's try split\n",
    "\n",
    "ridge_reg_split = Ridge().fit(X_train, y_train)\n",
    "mean_squared_error(y_test, ridge_reg_split.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# A little better (to same test split w/OLS) - can we improve it further?\n",
    "# We just went with defaults, but as always there's plenty of parameters\n",
    "help(Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to tune alpha? For now, let's loop and try values.\n",
    "\n",
    "For a stretch goal, check out [cross-validation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = []\n",
    "mses = []\n",
    "\n",
    "for alpha in range(0, 200, 1):\n",
    "    ridge_reg_split = Ridge(alpha=alpha).fit(X_train, y_train)\n",
    "    mse = mean_squared_error(y_test, ridge_reg_split.predict(X_test))\n",
    "    print(alpha, mse)\n",
    "    alphas.append(alpha)\n",
    "    mses.append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(alphas, mses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the intuition? What are we doing?\n",
    "\n",
    "The `alpha` parameter corresponds to the weight being given to the extra penalty being calculated by [Tikhonov regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization) (this parameter is sometimes referred to as $\\lambda$ in the context of ridge regression).\n",
    "\n",
    "Normal linear regression (OLS) minimizes the **sum of square error of the residuals**.\n",
    "\n",
    "Ridge regression minimizes the **sum of square error of the residuals** *AND* **the squared slope of the fit model, times the alpha parameter**.\n",
    "\n",
    "This is why the MSE for the first model in the for loop (`alpha=0`) is the same as the MSE for linear regression - it's the same model!\n",
    "\n",
    "As `alpha` is increased, we give more and more penalty to a steep slope. In two or three dimensions this is fairly easy to visualize - beyond, think of it as penalizing coefficient size. Each coefficient represents the slope of an individual dimension (feature) of the model, so ridge regression is just squaring and summing those.\n",
    "\n",
    "So while `alpha=0` reduces to OLS, as `alpha` approaches infinity eventually the penalty gets so extreme that the model will always output every coefficient as 0 (any non-zero coefficient resulting in a penalty that outweighs whatever improvement in the residuals), and just fit a flat model with intercept at the mean of the dependent variable.\n",
    "\n",
    "Of course, what we want is somewhere in-between these extremes. Intuitively, what we want to do is apply an appropriate \"cost\" or penalty to the model for fitting parameters, much like adjusted $R^2$ takes into account the cost of adding complexity to a model. What exactly is an appropriate penalty will vary, so you'll have to put on your model comparison hat and give it a go!\n",
    "\n",
    "PS - scaling the data helps, as that way this cost is consistent and can be added uniformly across features, and it is simpler to search for the `alpha` parameter.\n",
    "\n",
    "### Bonus - magic! ✨\n",
    "\n",
    "Ridge regression doesn't just reduce overfitting and help with the third aspect of well-posed problems (poor generalizability). It can also fix the first two (no unique solution)!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tiny = df.sample(10, random_state=27)\n",
    "print(df_tiny.shape)\n",
    "X = df_tiny.drop('Price', axis='columns')\n",
    "y = df_tiny.Price\n",
    "\n",
    "lin_reg = LinearRegression().fit(X, y)\n",
    "lin_reg.score(X, y)  # Perfect multi-collinearity!\n",
    "# NOTE - True OLS would 💥 here\n",
    "# scikit protects us from actual error, but still gives a poor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg = Ridge().fit(X, y)\n",
    "ridge_reg.score(X, y)  # More plausible (not \"perfect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our earlier test split\n",
    "mean_squared_error(y_test, lin_reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Ridge generalizes *way* better (and we've not even tuned alpha)\n",
    "mean_squared_error(y_test, ridge_reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And a bit more math\n",
    "\n",
    "The regularization used by Ridge Regression is also known as **$L^2$ regularization**, due to the squaring of the slopes being summed. This corresponds to [$L^2$ space](https://en.wikipedia.org/wiki/Square-integrable_function), a metric space of square-integrable functions that generally measure what we intuitively think of as \"distance\" (at least, on a plane) - what is referred to as Euclidean distance.\n",
    "\n",
    "The other famous norm is $L^1$, also known as [taxicab geometry](https://en.wikipedia.org/wiki/Taxicab_geometry), because it follows the \"grid\" to measure distance like a car driving around city blocks (rather than going directly like $L^2$). When referred to as a distance this is called \"Manhattan distance\", and can be used for regularization (see [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics%29), which [uses the $L^1$ norm](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization-How-does-it-solve-the-problem-of-overfitting-Which-regularizer-to-use-and-when)).\n",
    "\n",
    "All this comes down to - regularization means increasing model bias by \"watering down\" coefficients with a penalty typically based on some sort of distance metric, and thus reducing variance (overfitting the model to the noise in the data). It gives us another lever to try and another tool for our toolchest!\n",
    "\n",
    "### Putting it all together - one last example\n",
    "\n",
    "The official scikit-learn documentation has many excellent examples - [this one](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_ridge_variance.html#sphx-glr-auto-examples-linear-model-plot-ols-ridge-variance-py) illustrates how ridge regression effectively reduces the variance, again by increasing the bias, penalizing coefficients to reduce the effectiveness of features (but also the impact of noise)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Due to the few points in each dimension and the straight line that linear regression uses to follow these points as well as it can, noise on the observations will cause great variance as shown in the first plot. Every line’s slope can vary quite a bit for each prediction due to the noise induced in the observations.\n",
    "\n",
    "Ridge regression is basically minimizing a penalised version of the least-squared function. The penalising shrinks the value of the regression coefficients. Despite the few data points in each dimension, the slope of the prediction is much more stable and the variance in the line itself is greatly reduced, in comparison to that of the standard linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "X_train = np.c_[.5, 1].T\n",
    "y_train = [.5, 1]\n",
    "X_test = np.c_[0, 2].T\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "classifiers = dict(ols=linear_model.LinearRegression(),\n",
    "                   ridge=linear_model.Ridge(alpha=.1))\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "    for _ in range(6):\n",
    "        this_X = .1 * np.random.normal(size=(2, 1)) + X_train\n",
    "        clf.fit(this_X, y_train)\n",
    "\n",
    "        ax.plot(X_test, clf.predict(X_test), color='gray')\n",
    "        ax.scatter(this_X, y_train, s=3, c='gray', marker='o', zorder=10)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    ax.plot(X_test, clf.predict(X_test), linewidth=2, color='blue')\n",
    "    ax.scatter(X_train, y_train, s=30, c='red', marker='+', zorder=10)\n",
    "\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlim(0, 2)\n",
    "    ax.set_ylim((0, 1.6))\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "In your assignment, you will fit a Ridge Regression model. You can try Linear Regression too — depending on how many features you select, your errors will probably blow up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "For your assignment, we're going back to our other **New York City** real estate dataset. Instead of predicting apartment rents, you'll predict property sales prices.\n",
    "\n",
    "But not just for condos in Tribeca...\n",
    "\n",
    "Instead, predict property sales prices for **One Family Dwellings** (`BUILDING_CLASS_CATEGORY` == `'01 ONE FAMILY DWELLINGS'`). \n",
    "\n",
    "Use a subset of the data where the **sale price was more than \\\\$100 thousand and less than $2 million.** \n",
    "\n",
    "You'll practice all of the module's objectives to build your best model yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "Resources:\n",
    "- https://www.quora.com/What-is-regularization-in-machine-learning\n",
    "- https://blogs.sas.com/content/subconsciousmusings/2017/07/06/how-to-use-regularization-to-prevent-model-overfitting/\n",
    "- https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/\n",
    "- https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\n",
    "- https://stats.stackexchange.com/questions/111017/question-about-standardizing-in-ridge-regression#111022"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
